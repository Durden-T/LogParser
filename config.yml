name: hadoop
levels: # 输入日志的级别范围
  - default
  - info
  - error
  - warn
  - fatal
compress_state: false # 压缩保存state
snapshot_duration: 600s # 保存state的间隔
persistence_file_path: save.txt # 持久化文件名

max_depth: 4 #解析树最大深度,这三个值可以固定，也可以手动调比较结果，玄学
similarity_threshold: 0.4 # 相似度阈值
max_children: 100 # 叶节点孩子数的最大数量

# 掩码，替换输入日志中的某种模式，将匹配到regex_pattern的内容用mask_with替换，这个masking分别将IP与数字替换为了IP，NUM。领域知识预处理，提高精度
masking: '[
              {"regex_pattern":"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}(:\\d{1,5})?", "mask_with": "<IP>" },
              { "regex_pattern": "[\\-\\+]?\\d+", "mask_with": "<NUM>" }
            ]'

kafka:
  hosts:
    - 10.128.244.250:9092
  input_topic: hadoop_log_input # 原始日志的输入topic
  output_topic: hadoop_log_output # 解析后的日志模板输出topic
  read_min_bytes: 10e3 # 10kb 每次最少读多少
  read_max_bytes: 10e6 # 10mb # 每次最多都多少
  commit_interval: 5s  # flushes commits to Kafka every 5 second 5s commit一次
  write_batch_size: 100e3 # 100kb # 写批处理的个数
  write_batch_bytes: 10e6 #10mb # 写批处理的大小
  write_batch_timeout: 1s # 实际使用batch timeout控制, 每秒写一次

mysql:
  path: '127.0.0.1:3306'
  config: 'charset=utf8mb4&parseTime=True&loc=Local'
  db-name: 'log_analysis_management'
  username: 'root'
  password: 'k!2a7Lrn3'
  table-name: 'hadoop_log_templates'

save_result_duration: 1s # 保存结果的duration

